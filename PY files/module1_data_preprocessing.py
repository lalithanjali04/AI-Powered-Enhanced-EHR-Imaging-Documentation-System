# -*- coding: utf-8 -*-
"""Module1_Data_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f8YkI_5VqeCyzGcSBjnE7iFbroTTqJ8Q

# AI-Powered-Enhanced EHR Imaging & Documentation System (Module 1 )
"""

!ls /content

# --- Unzip Doctor‚Äôs Handwritten Prescription BD dataset ---
!unzip "/content/Doctor‚Äôs Handwritten Prescription BD dataset.zip" -d "/content/Doctor_Prescriptions/"

# --- Unzip Xrays dataset ---
!unzip "/content/XraysData.zip" -d "/content/xray_data/"

!ls /content/Doctor_Prescriptions
!ls /content/xray_data

"""# ü©∫ AI-Powered Enhanced EHR Imaging & Documentation System
### Module 1 ‚Äì Data Collection & Preprocessing  

This module focuses on collecting, cleaning, and preparing both **structured (tabular)** and **unstructured (image/text)** healthcare data for further AI-based processing.  
The datasets used are:  

- **Healthcare Dataset** ‚Äì patient and hospital details  
- **ICD CodeSet** ‚Äì medical code‚Äìdisease mapping  
- **Doctor‚Äôs Handwritten Prescription BD Dataset** ‚Äì image-based prescription data  
- **X-ray Data** ‚Äì medical imaging dataset  

The goal is to perform:
- Data cleaning, encoding, and scaling for structured data  
- Indexing and linking of unstructured datasets  
- Saving the preprocessed outputs for later modules (Image Enhancement & Diagnosis Prediction)

## ‚öôÔ∏è Step 1 ‚Äì Environment Setup  
This step imports all the required Python libraries such as `pandas`, `numpy`, `matplotlib`, `seaborn`, and scikit-learn preprocessing modules.  
These libraries help in data handling, visualization, encoding, and scaling.
"""

# =====================================================================
#  MODULE 1: DATA COLLECTION & PREPROCESSING (FINAL VERSION)
# Project: AI-Powered Enhanced EHR Imaging & Documentation System
# =====================================================================

# --- 1Ô∏è. SETUP ---
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler

sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (8, 4)

# Mount Google Drive if needed
# from google.colab import drive
# drive.mount('/content/drive')

"""## üìÇ Step 2 ‚Äì Dataset Loading  
Here we load the following CSV datasets:  
- `healthcare_dataset.csv`  
- `ICDCodeSet.csv`  

After loading, we inspect their shape and preview the first few records to ensure successful import.  
If a `patient_id` column is missing, we create one for unique identification.  

"""

# =====================================================================
# 2Ô∏è. LOAD DATASETS
# =====================================================================

healthcare_path = "/content/healthcare_dataset.csv"
icd_path        = "/content/ICDCodeSet.csv"

print("=== Loading Healthcare Dataset ===")
df_health = pd.read_csv(healthcare_path)
print("Healthcare shape:", df_health.shape)
display(df_health.head())

print("\n=== Loading ICD Code Dataset ===")
df_icd = pd.read_csv(icd_path)
print("ICD shape:", df_icd.shape)
display(df_icd.head())

# Add patient_id if missing
if 'patient_id' not in df_health.columns:
    df_health.insert(0, 'patient_id', range(1001, 1001 + len(df_health)))
    print("üÜî patient_id added.")

"""## üßπ Step 3 ‚Äì Healthcare Dataset Cleaning & Preprocessing  
In this step we:
1. Check for missing values  
2. Convert date columns into a new numerical feature **Stay_Duration**  
3. Drop irrelevant fields such as Name, Doctor, Hospital, etc.  
4. Handle missing data using median/mode imputation  
5. Encode categorical columns (Gender, Blood Type, Medical Condition, etc.)  
6. Scale numerical features using `StandardScaler`  

The resulting dataset becomes completely numeric and ready for machine-learning processing.  

"""

# =====================================================================
# 3Ô∏è. CLEAN & PREPROCESS HEALTHCARE DATASET
# =====================================================================

print("\n=== Missing Value Check ===")
print(df_health.isnull().sum())

# Convert dates ‚Üí Stay_Duration
if 'Date of Admission' in df_health.columns and 'Discharge Date' in df_health.columns:
    df_health['Date of Admission'] = pd.to_datetime(df_health['Date of Admission'], errors='coerce')
    df_health['Discharge Date']    = pd.to_datetime(df_health['Discharge Date'], errors='coerce')
    df_health['Stay_Duration'] = (df_health['Discharge Date'] - df_health['Date of Admission']).dt.days
    print("üìÖ Stay_Duration created.")

# Drop non-informative columns
cols_to_drop = [
    'Name', 'Doctor', 'Hospital',
    'Insurance Provider', 'Billing Amount',
    'Room Number', 'Date of Admission', 'Discharge Date'
]
df_health.drop(columns=cols_to_drop, inplace=True, errors='ignore')
print("üßπ Dropped non-informative columns.")

# Handle missing values
df_health = df_health.fillna(df_health.median(numeric_only=True))
df_health = df_health.fillna(df_health.mode().iloc[0])
print("‚úÖ Missing values handled.")

# Encode only selected categorical columns
cols_to_encode = [
    'Gender', 'Blood Type',
    'Medical Condition', 'Admission Type',
    'Medication', 'Test Results'
]

label_mappings = {}
for col in cols_to_encode:
    if col in df_health.columns:
        le = LabelEncoder()
        df_health[col] = le.fit_transform(df_health[col].astype(str))
        label_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))

print("\nüî¢ Label Encodings Applied:")
for col, mapping in label_mappings.items():
    print(f"{col}: {mapping}")

# Scale numeric columns
scaler = StandardScaler()
num_cols = df_health.select_dtypes(include=['int64','float64']).columns.drop('patient_id', errors='ignore')
df_health[num_cols] = scaler.fit_transform(df_health[num_cols])
print("üìä Numeric columns scaled.")

print("\n‚úÖ Cleaned Healthcare Dataset Preview:")
display(df_health.head())

"""## üßæ Step 4 ‚Äì ICD-10 Code Dataset Preparation  
This step removes duplicates, standardizes column names, and creates a mapping between **ICD Code ‚Üí Description**.  
These codes will later be linked with patient diagnoses in Module 3.  

"""

# =====================================================================
# 4Ô∏è. CLEAN ICD CODE DATASET
# =====================================================================

df_icd = df_icd.drop_duplicates()
df_icd.columns = [c.strip().replace(" ", "_") for c in df_icd.columns]
icd_mapping = dict(zip(df_icd.iloc[:,0], df_icd.iloc[:,1]))

"""## üíä Step 5 ‚Äì Doctor‚Äôs Handwritten Prescription Dataset Processing  
Each split (train, test, validation) contains an image folder and a label CSV.  
This block:
- Loads the label CSVs  
- Adds split identifiers  
- Builds proper image paths  
- Creates unique `prescription_id`s  

The three splits are then concatenated into one DataFrame.  

"""

# =====================================================================
# 5Ô∏è. PREPROCESS PRESCRIPTION DATASET
# =====================================================================

pres_train_path = "/content/Doctor_Prescriptions/Doctor‚Äôs Handwritten Prescription BD dataset/Training/training_labels.csv"
pres_test_path  = "/content/Doctor_Prescriptions/Doctor‚Äôs Handwritten Prescription BD dataset/Testing/testing_labels.csv"
pres_val_path   = "/content/Doctor_Prescriptions/Doctor‚Äôs Handwritten Prescription BD dataset/Validation/validation_labels.csv"

def preprocess_prescription_csv(path, split_name):
    if not os.path.exists(path):
        print(f"‚ö†Ô∏è {path} file not found, skipping.")
        return pd.DataFrame()
    df = pd.read_csv(path)
    df.columns = [c.strip().replace(" ", "_") for c in df.columns]
    df['split'] = split_name
    # Correct the image path assuming image files are in a subdirectory named after the split
    df['image_path'] = df['IMAGE'].apply(lambda x: f"/content/Doctor_Prescriptions/Doctor‚Äôs Handwritten Prescription BD dataset/{split_name}/{split_name}_words/{x}")
    df['prescription_id'] = [f"P{1000+i}" for i in range(len(df))]
    return df

df_pres_train = preprocess_prescription_csv(pres_train_path, "Training")
df_pres_test  = preprocess_prescription_csv(pres_test_path, "Testing")
df_pres_val   = preprocess_prescription_csv(pres_val_path, "Validation")


df_pres = pd.concat([df_pres_train, df_pres_test, df_pres_val], ignore_index=True)
print("Prescription data shape:", df_pres.shape)
display(df_pres.head())

import pandas as pd

pres_train_path = "/content/Doctor_Prescriptions/Doctor‚Äôs Handwritten Prescription BD dataset/Training/training_labels.csv"

try:
    df_train_sample = pd.read_csv(pres_train_path)
    print("Columns in training_labels.csv:")
    print(df_train_sample.columns)
except FileNotFoundError:
    print(f"‚ö†Ô∏è {pres_train_path} not found.")

"""## ü©ª Step 6 ‚Äì X-ray Image Dataset Indexing  
Here we scan the X-ray image folders (`train`, `test`, `val`) and record:
- File paths  
- Class labels (adenocarcinoma, large_cell, squamous_cell, normal)  
- Split type  

A single DataFrame is created to serve as an image index for further enhancement tasks.  

"""

# =====================================================================
# 6Ô∏è. PREPROCESS X-RAY DATASET
# =====================================================================

xray_root = "/content/xray_data/XraysData"  # update path

xray_records = []
for split in ["train", "test", "val"]:
    split_path = os.path.join(xray_root, split)
    if not os.path.exists(split_path): continue
    for cls in os.listdir(split_path):
        class_dir = os.path.join(split_path, cls)
        if os.path.isdir(class_dir):
            for img in os.listdir(class_dir):
                if img.lower().endswith(('.jpg','.png')):
                    xray_records.append({
                        'xray_id': f"X{len(xray_records)+1:04d}",
                        'split': split,
                        'label': cls,
                        'image_path': os.path.join(class_dir, img)
                    })

df_xray = pd.DataFrame(xray_records)
print("X-ray data shape:", df_xray.shape)
display(df_xray.head())

"""## üîó Step 7 ‚Äì Creating Synthetic Links Between Datasets  
To simulate real EHR connectivity, we randomly assign:
- `prescription_id` from the prescription dataset  
- `xray_id` from the X-ray index  

This ensures each patient record can be connected to its medical images and prescriptions.  

"""

# =====================================================================
# 7Ô∏è. CREATE SYNTHETIC LINKS BETWEEN DATASETS
# =====================================================================

if not df_pres.empty:
    df_health['prescription_id'] = np.random.choice(df_pres['prescription_id'], size=len(df_health))
if not df_xray.empty:
    df_health['xray_id'] = np.random.choice(df_xray['xray_id'], size=len(df_health))

"""## üíæ Step 8 ‚Äì Saving Cleaned Datasets  
All cleaned and indexed datasets are exported into the `/content/cleaned_data/` directory for reuse in subsequent modules:  
- `cleaned_healthcare_dataset.csv`  
- `cleaned_icdcodeset.csv`  
- `cleaned_prescription_dataset.csv`  
- `cleaned_xray_index.csv`  

"""

# =====================================================================
# 8Ô∏è. SAVE CLEANED DATASETS
# =====================================================================

os.makedirs("/content/cleaned_data", exist_ok=True)
df_health.to_csv("/content/cleaned_data/cleaned_healthcare_dataset.csv", index=False)
df_icd.to_csv("/content/cleaned_data/cleaned_icdcodeset.csv", index=False)
if not df_pres.empty:
    df_pres.to_csv("/content/cleaned_data/cleaned_prescription_dataset.csv", index=False)
if not df_xray.empty:
    df_xray.to_csv("/content/cleaned_data/cleaned_xray_index.csv", index=False)

print("üíæ All cleaned datasets saved in /content/cleaned_data")

"""## üìä Step 9 ‚Äì Module 1 Summary  
This cell prints final dataset shapes and confirms the preprocessing pipeline executed successfully.  
The outputs of Module 1 will be used in Module 2 (Image Enhancement) and Module 3 (Disease Prediction & Documentation).  

"""

# =====================================================================
# 9Ô∏è. SUMMARY
# =====================================================================

print("\n=== MODULE 1 SUMMARY ===")
print(f"Healthcare records: {df_health.shape}")
print(f"ICD codes:          {df_icd.shape}")
print(f"Prescription data:  {df_pres.shape}")
print(f"X-ray data:         {df_xray.shape}")
print("\nüéØ Module 1 completed successfully ‚Äî ready for Module 2 (Image Enhancement)!")

"""## üîç Step 10 ‚Äì EDA & Validation of Cleaned Datasets  
We now verify:
- Missing or duplicate entries  
- Correct encoding and scaling  
- Data distributions and correlations  
- Dataset linkage consistency  

Visualizations (count plots, heatmaps) confirm that all preprocessing steps were successful.  


"""

# =====================================================================
# üîç MODULE 1 VALIDATION & EDA (Exploratory Data Analysis)
# =====================================================================

import seaborn as sns
import matplotlib.pyplot as plt

# ---------------------------------------------------------------------
# 1Ô∏è‚É£ HEALTHCARE DATASET EDA
# ---------------------------------------------------------------------
print("\n=== ü©∫ HEALTHCARE DATASET VALIDATION ===")

# Basic info
print(df_health.info())
print("\n‚úÖ Missing Values per Column:\n", df_health.isnull().sum())
print("\n‚úÖ Duplicate Rows:", df_health.duplicated().sum())

# Summary statistics
print("\nüìä Numerical Summary:")
display(df_health.describe())

# Check unique counts for categorical columns
cat_cols = ['Gender','Blood Type','Medical Condition','Admission Type','Medication','Test Results']
for col in cat_cols:
    if col in df_health.columns:
        print(f"\nüß© Unique values in {col}: {df_health[col].unique()}")


# Create a temporary DataFrame for visualization using original categorical values
df_health_viz = df_health.copy()

# Revert label encoding for visualization purposes
reverse_label_mappings = {col: {v: k for k, v in mapping.items()} for col, mapping in label_mappings.items()}

for col in cat_cols:
    if col in df_health_viz.columns and col in reverse_label_mappings:
        df_health_viz[col] = df_health_viz[col].map(reverse_label_mappings[col])


# Visualization 1: Gender Distribution
plt.figure(figsize=(5,4))
sns.countplot(x='Gender', data=df_health_viz, hue='Gender', legend=False) # Added hue and legend=False, removed palette
plt.title("Gender Distribution in Healthcare Dataset") # Added more specific title
plt.xlabel("Gender") # Ensure clear label
plt.ylabel("Count") # Ensure clear label
plt.show()

# Visualization 2: Medical Condition Distribution
plt.figure(figsize=(12, 7)) # Increased figure size slightly
sns.countplot(x='Medical Condition', data=df_health_viz, hue='Medical Condition', legend=False) # Added hue and legend=False, removed palette
plt.title("Distribution of Medical Conditions in Healthcare Dataset") # Added more specific title
plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
plt.xlabel("Medical Condition") # Ensure clear label
plt.ylabel("Count") # Ensure clear label
plt.tight_layout() # Adjust layout to prevent labels overlapping
plt.show()

# Visualization 3: Test Results
plt.figure(figsize=(6,4))
sns.countplot(x='Test Results', data=df_health_viz, hue='Test Results', legend=False) # Added hue and legend=False, removed palette
plt.title("Test Results Distribution in Healthcare Dataset") # Added more specific title
plt.xlabel("Test Results") # Ensure clear label
plt.ylabel("Count") # Ensure clear label
plt.show()

# Visualization 4: Admission Type Distribution
plt.figure(figsize=(6,4))
sns.countplot(x='Admission Type', data=df_health_viz, hue='Admission Type', legend=False) # Added hue and legend=False
plt.title("Admission Type Distribution in Healthcare Dataset") # Added more specific title
plt.xlabel("Admission Type") # Ensure clear label
plt.ylabel("Count") # Ensure clear label
plt.show()


# Visualization 5: Correlation Heatmap
plt.figure(figsize=(10,8)) # Increased figure size
sns.heatmap(df_health[num_cols].corr(), annot=True, cmap="coolwarm", fmt=".2f") # Added fmt for annotation format
plt.title("Feature Correlation Heatmap (Scaled Numeric Features)") # Added more specific title
plt.show()

# ---------------------------------------------------------------------
# 2Ô∏è‚É£ ICD DATASET CHECK
# ---------------------------------------------------------------------
print("\n=== üßæ ICD CODE DATASET VALIDATION ===")
print("Shape:", df_icd.shape)
print("‚úÖ Duplicates:", df_icd.duplicated().sum())
print("‚úÖ Missing values:", df_icd.isnull().sum().sum())
print("\nSample ICD Records:")
display(df_icd.head())

# Check for any empty descriptions
if df_icd.iloc[:,1].isnull().any():
    print("‚ö†Ô∏è Some ICD descriptions missing!")
else:
    print("‚úÖ All ICD descriptions are present.")

# ---------------------------------------------------------------------
# 3Ô∏è‚É£ PRESCRIPTION DATASET CHECK
# ---------------------------------------------------------------------
print("\n=== üíä PRESCRIPTION DATASET VALIDATION ===")

if not df_pres.empty:
    print("Shape:", df_pres.shape)
    print("‚úÖ Missing Values:", df_pres.isnull().sum().sum())
    print("‚úÖ Duplicate Records:", df_pres.duplicated().sum())
    print("\nSample Prescription Records:")
    display(df_pres.head())

    # Count by split
    plt.figure(figsize=(5,4))
    sns.countplot(x='split', data=df_pres)
    plt.title("Prescription Data Distribution by Split")
    plt.show()

    # Check label frequencies
    if 'label' in df_pres.columns:
        plt.figure(figsize=(6,4))
        sns.countplot(y='label', data=df_pres, order=df_pres['label'].value_counts().index)
        plt.title("Prescription Label Frequency")
        plt.show()

else:
    print("‚ö†Ô∏è Prescription dataset empty or not loaded.")

# ---------------------------------------------------------------------
# 4Ô∏è‚É£ X-RAY DATASET CHECK
# ---------------------------------------------------------------------
print("\n=== ü©ª X-RAY DATASET VALIDATION ===")

if not df_xray.empty:
    print("Shape:", df_xray.shape)
    print("‚úÖ Missing Values:", df_xray.isnull().sum().sum())
    print("‚úÖ Duplicate Records:", df_xray.duplicated().sum())
    print("\nSample X-ray Records:")
    display(df_xray.head())

    # Count by split
    plt.figure(figsize=(5,4))
    sns.countplot(x='split', data=df_xray)
    plt.title("X-ray Data Distribution by Split")
    plt.show()

    # Count by label (disease class)
    plt.figure(figsize=(7,4))
    sns.countplot(y='label', data=df_xray, order=df_xray['label'].value_counts().index)
    plt.title("X-ray Label Distribution")
    plt.show()
else:
    print("‚ö†Ô∏è X-ray dataset empty or not loaded.")

# ---------------------------------------------------------------------
# 5Ô∏è‚É£ CROSS-DATA VALIDATION
# ---------------------------------------------------------------------
print("\n=== üîó CROSS-DATA LINK VALIDATION ===")

# Ensure that prescription_id and xray_id exist in linked datasets
if 'prescription_id' in df_health.columns:
    linked_pres = df_health['prescription_id'].isin(df_pres['prescription_id']).sum()
    print(f"Linked Prescription IDs found: {linked_pres}/{len(df_health)}")

if 'xray_id' in df_health.columns:
    linked_xrays = df_health['xray_id'].isin(df_xray['xray_id']).sum()
    print(f"Linked X-ray IDs found: {linked_xrays}/{len(df_health)}")

print("\n‚úÖ All datasets cleaned and relationships verified successfully!")

"""# ‚úÖ Module 1 Completed Successfully  

**Summary:**  
- Structured & unstructured data collected and cleaned  
- Unique identifiers added for cross-referencing  
- Data standardized, encoded, and scaled  
- Cleaned datasets saved for further use  

Next, we proceed to **Module 2 ‚Äì Medical Image Enhancement** to improve X-ray clarity using pretrained AI models.  

"""